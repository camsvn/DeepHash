{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task4_semihardNegetive.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9u5OIoawwId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD93VGcow0WB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "num_workers = 1\n",
        "\n",
        "#define transforms\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# load imagenet\n",
        "image_dataset = {\n",
        "        'train' :datasets.CIFAR10('./', train=True, download=True, transform=data_transforms['train']),\n",
        "        'test' : datasets.CIFAR10('./', train=False, download=True, transform=data_transforms['train']) \n",
        "}\n",
        "\n",
        "# Create the dataloaders\n",
        "data_loader = {\n",
        "    'train': torch.utils.data.DataLoader(image_dataset['train'], batch_size=batch_size, shuffle=True, num_workers=num_workers),\n",
        "    'test': torch.utils.data.DataLoader(image_dataset['test'], batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j7_JBDlw6zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# An identity layer to pass the fc layer in resnet\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x\n",
        "  \n",
        "# Define model\n",
        "resnet18  = models.resnet18(pretrained=True)\n",
        "resnet18.fc = Identity()\n",
        "\n",
        "# Freeze all the parameters in the model\n",
        "def freeze_model(model):\n",
        "  for params in model.parameters():\n",
        "    params.requires_grad=False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GOWnJuCBno7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/jjmachan/DeepHash\n",
        "!mv siamese-triplet triplet\n",
        "import triplet.datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHv-sl5Qy9g-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from triplet.trainer import fit\n",
        "import numpy as np\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5yr_XVCBwrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class TripletCifar1(Dataset):\n",
        "    \"\"\"\n",
        "    Train: For each sample (anchor) randomly chooses a positive and negative samples\n",
        "    Test: Creates fixed triplets for testing\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.train = self.dataset.train\n",
        "        self.transform = self.dataset.transform\n",
        "\n",
        "        if self.train:\n",
        "            self.train_labels = np.array(self.dataset.targets,dtype=np.float32)\n",
        "            self.train_data = self.dataset.data\n",
        "            #self.train_data = self.train_data.reshape(-1, 3, 32, 32)\n",
        "            #self.train_data = self.train_data.transpose((0, 2, 3, 1))  # convert to HWC\n",
        "            self.labels_set = set(self.train_labels)\n",
        "            self.label_to_indices = {label: np.where(self.train_labels == label)[0]\n",
        "                                     for label in self.labels_set}\n",
        "        else:\n",
        "            self.test_labels = np.array(self.dataset.targets)\n",
        "            self.test_data = self.dataset.data\n",
        "            # generate fixed triplets for testing\n",
        "            self.labels_set = set(self.test_labels)\n",
        "            self.label_to_indices = {label: np.where(self.test_labels == label)[0]\n",
        "                                     for label in self.labels_set}\n",
        "\n",
        "            random_state = np.random.RandomState(9)\n",
        "\n",
        "            triplets = [[i,\n",
        "                         random_state.choice(self.label_to_indices[self.test_labels[i].item()]),\n",
        "                         random_state.choice(self.label_to_indices[\n",
        "                                                 np.random.choice(\n",
        "                                                     list(self.labels_set - set([self.test_labels[i].item()]))\n",
        "                                                 )\n",
        "                                             ])\n",
        "                         ]\n",
        "                        for i in range(len(self.test_data))]\n",
        "            self.test_triplets = triplets\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.train:\n",
        "            img1, label1 = self.train_data[index], self.train_labels[index].item()\n",
        "            positive_index = index\n",
        "            while positive_index == index:\n",
        "                positive_index = np.random.choice(self.label_to_indices[label1])\n",
        "            negative_label = np.random.choice(list(self.labels_set - set([label1])))\n",
        "            negative_index = np.random.choice(self.label_to_indices[negative_label])\n",
        "            img2 = self.train_data[positive_index]\n",
        "            img3 = self.train_data[negative_index]\n",
        "        else:\n",
        "            img1 = self.test_data[self.test_triplets[index][0]]\n",
        "            img2 = self.test_data[self.test_triplets[index][1]]\n",
        "            img3 = self.test_data[self.test_triplets[index][2]]\n",
        "\n",
        "        img1 = Image.fromarray(img1)\n",
        "        img2 = Image.fromarray(img2)\n",
        "        img3 = Image.fromarray(img3)\n",
        "        if self.transform is not None:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "            img3 = self.transform(img3)\n",
        "        return (img1, img2, img3), []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieaDXrLeImS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.sampler import BatchSampler\n",
        "\n",
        "class BalancedBatchSampler1(BatchSampler):\n",
        "    \"\"\"\n",
        "    BatchSampler - from a MNIST-like dataset, samples n_classes and within these classes samples n_samples.\n",
        "    Returns batches of size n_classes * n_samples\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, labels, n_classes, n_samples):\n",
        "        self.labels = np.array(labels, dtype=np.float32)\n",
        "        self.labels_set = list(set(self.labels))\n",
        "        self.label_to_indices = {label: np.where(self.labels == label)[0]\n",
        "                                 for label in self.labels_set}\n",
        "        for l in self.labels_set:\n",
        "            np.random.shuffle(self.label_to_indices[l])\n",
        "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
        "        self.count = 0\n",
        "        self.n_classes = n_classes\n",
        "        self.n_samples = n_samples\n",
        "        self.n_dataset = len(self.labels)\n",
        "        self.batch_size = self.n_samples * self.n_classes\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = 0\n",
        "        while self.count + self.batch_size < self.n_dataset:\n",
        "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
        "            indices = []\n",
        "            for class_ in classes:\n",
        "                indices.extend(self.label_to_indices[class_][\n",
        "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
        "                                                                         class_] + self.n_samples])\n",
        "                self.used_label_indices_count[class_] += self.n_samples\n",
        "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
        "                    np.random.shuffle(self.label_to_indices[class_])\n",
        "                    self.used_label_indices_count[class_] = 0\n",
        "            yield indices\n",
        "            self.count += self.n_classes * self.n_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_dataset // self.batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xnXFaWMlxnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up data loaders\n",
        "from triplet.datasets import BalancedBatchSampler\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "train_batch_sampler = BalancedBatchSampler1(image_dataset['train'].targets, n_classes=10, n_samples=25)\n",
        "test_batch_sampler = BalancedBatchSampler1(image_dataset['test'].targets, n_classes=10, n_samples=25)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg0n98HUJMp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
        "online_train_loader = torch.utils.data.DataLoader(image_dataset['train'], batch_sampler=train_batch_sampler, **kwargs)\n",
        "online_test_loader = torch.utils.data.DataLoader(image_dataset['test'], batch_sampler=test_batch_sampler, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR3nVDnvMdLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loader = iter(online_train_loader)\n",
        "images, labels = next(loader)\n",
        "print(images.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InBCTR7sLvrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up the network and training parameters\n",
        "from triplet.networks import EmbeddingNet\n",
        "from triplet.losses import OnlineTripletLoss\n",
        "from triplet.utils import AllTripletSelector,HardestNegativeTripletSelector, RandomNegativeTripletSelector, SemihardNegativeTripletSelector # Strategies for selecting triplets within a minibatch\n",
        "from triplet.metrics import AverageNonzeroTripletsMetric\n",
        "\n",
        "margin = 1.\n",
        "embedding_net = resnet18\n",
        "model = embedding_net\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "loss_fn = OnlineTripletLoss(margin, SemihardNegativeTripletSelector(margin))\n",
        "lr = 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
        "n_epochs = 20\n",
        "log_interval = 15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUkHpGiVl1JX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fit(online_train_loader, online_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[AverageNonzeroTripletsMetric()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzxfPC3avk-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model, './triplet_resnet18_SemihardNegetiveTripletSelector.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtqvO2WPKUnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = OnlineTripletLoss(margin, HardestNegativeTripletSelector(margin))\n",
        "unfreeze_model(model)\n",
        "fit(online_train_loader, online_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[AverageNonzeroTripletsMetric()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD21WqXCHlqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model, './triplet_vgg20_2.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddYXN5naIdXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kodDsXoMwF6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save to google drive \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "!cp *.mdl /gdrive/My\\ Drive/tooploox"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP3tTynrwS_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!cp /gdrive/My\\ Drive/tooploox/*.mdl ./ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj5dM0U2v4f9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_test = torch.load('./triplet_vgg19.mdl')\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOw-fCVY_sGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Freeze all the parameters in the model\n",
        "def freeze_model(model):\n",
        "  for params in model.parameters():\n",
        "    params.requires_grad=False\n",
        "\n",
        "def unfreeze_model(model):\n",
        "  for params in model.parameters():\n",
        "    params.requires_grad= True\n",
        "\n",
        "# check if all the parameters have been freezed\n",
        "def list_trainable(model):\n",
        "  for params in model.parameters():\n",
        "    print(params.requires_grad)\n",
        "  \n",
        "# delete the last layers\n",
        "def del_last_layers(model_class, num_layers):\n",
        "  model_class = nn.Sequential(*list(model_class.children())[:-num_layers])\n",
        "  return model_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6-64Pswx7GH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_embeddings(model, embedding_size):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  print(device)\n",
        "\n",
        "  features = {}\n",
        "  targets = {}\n",
        "  model.to(device)\n",
        "  features['train'] = np.empty([0, embedding_size])\n",
        "  targets['train'] = np.empty([0, ])\n",
        "\n",
        "  features['test'] = np.empty([0, embedding_size])\n",
        "  targets['test'] = np.empty([0,])\n",
        "\n",
        "  for i, (images,target) in enumerate(data_loader['train']):\n",
        "    images = images.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    try:\n",
        "      output = model(images).cpu().numpy()\n",
        "      features['train'] = np.append(features['train'],output, axis=0)\n",
        "      targets['train'] = np.append(targets['train'],target.cpu(), axis=0)\n",
        "    except:\n",
        "      print(output.shape)\n",
        "      print('error occured: ', e)\n",
        "      return (None, None)\n",
        "    \n",
        "    if i%100 == 0:\n",
        "      print(i)\n",
        "\n",
        "  for i, (images,target) in enumerate(data_loader['test']):\n",
        "    images = images.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    output = model(images).cpu().numpy()\n",
        "    features['test'] = np.append(features['test'],output, axis=0)\n",
        "    targets['test'] = np.append(targets['test'],target.cpu(), axis=0)\n",
        "\n",
        "    if i%100 == 0:\n",
        "      print(i)\n",
        "  return (features, targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8_GyOjMylBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features, targets = create_embeddings(model_emb, 512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGjNaMPZyvWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_emb = model\n",
        "freeze_model(model_emb)\n",
        "print(model_emb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgGvI0PR_hlm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save and load\n",
        "import pickle\n",
        "\n",
        "def save(features, targets, model_name):\n",
        "  with open(model_name+'.embs', 'wb') as file:\n",
        "    pickle.dump((features,targets), file)\n",
        "    print('file saved in ', model_name)\n",
        "\n",
        "def load(model_name):\n",
        "  with open(model_name+'.embs', 'rb') as file:\n",
        "    features, targets = pickle.load(file)\n",
        "    return (features, targets)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Miqi61eqAIvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the computed embeddings\n",
        "save(features, targets, 'resnet18_semihardNegetive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6waqp5oATjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save to google drive \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "!cp *embs /gdrive/My\\ Drive/tooploox"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP891c-sYuQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls /gdrive/My\\ Drive/tooploox"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpDtuaMfAUms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "k_range = range(25,30)\n",
        "\n",
        "def search_knn_accuracies(k_range, features, targets):\n",
        "  acc = []\n",
        "  for k in k_range:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(features['train'], targets['train'])\n",
        "    #print('finished fitting')\n",
        "    predict = knn.predict(features['test'][:300,:])\n",
        "    #print('predicted')\n",
        "    score = metrics.accuracy_score(targets['test'][:300], predict)\n",
        "    print('K value: %d, accuracy: %0.7f' %(k, score))\n",
        "    acc.append(score)\n",
        "  print('Mean accuracy ',sum(acc)/len(acc))\n",
        "\n",
        "# the best score was obtained when k = 20:24\n",
        "search_knn_accuracies(k_range, features, targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFy1JSKSAZU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as PathEffects\n",
        "from sklearn.manifold import TSNE\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "sns.set_palette('muted')\n",
        "sns.set_context(\"notebook\", font_scale=1.5,\n",
        "                rc={\"lines.linewidth\": 2.5})\n",
        "RS = 123\n",
        "# need to create a subset of the data, too much time to process otherwise\n",
        "x_subset = features['train'][:5000]\n",
        "y_subset = targets['train'][:5000]\n",
        "\n",
        "print(np.unique(y_subset))\n",
        "labels = {\n",
        "     0: 'airplane',  \n",
        "     1: 'automobile',\n",
        "     2: 'bird',\n",
        "     3: 'cat',\n",
        "     4: 'deer',\n",
        "     5: 'dog',\n",
        "     6: 'frog',\n",
        "     7: 'horse',\n",
        "     8: 'ship',\n",
        "     9: 'truck',\n",
        "}\n",
        "# Utility function to visualize the outputs of PCA and t-SNE\n",
        "\n",
        "def fashion_scatter(x, colors):\n",
        "    # choose a color palette with seaborn.\n",
        "    num_classes = len(np.unique(colors))\n",
        "    palette = np.array(sns.color_palette(\"hls\", num_classes))\n",
        "\n",
        "    # create a scatter plot.\n",
        "    f = plt.figure(figsize=(8, 8))\n",
        "    ax = plt.subplot(aspect='equal')\n",
        "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=palette[colors.astype(np.int)])\n",
        "    plt.xlim(-25, 25)\n",
        "    plt.ylim(-25, 25)\n",
        "    ax.axis('off')\n",
        "    ax.axis('tight')\n",
        "\n",
        "    # add the labels for each digit corresponding to the label\n",
        "    txts = []\n",
        "\n",
        "    for i in range(num_classes):\n",
        "\n",
        "        # Position of each label at median of data points.\n",
        "\n",
        "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
        "        txt = ax.text(xtext, ytext, str(labels[i]), fontsize=24)\n",
        "        txt.set_path_effects([\n",
        "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
        "            PathEffects.Normal()])\n",
        "        txts.append(txt)\n",
        "\n",
        "    return f, ax, sc, txts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD-BsELyA6BR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# do pca before passing to tsne to reduce noice and fast performance\n",
        "time_start = time.time()\n",
        "\n",
        "pca_50 = PCA(n_components=50)\n",
        "pca_result_50 = pca_50.fit_transform(x_subset)\n",
        "\n",
        "print('PCA with 50 components done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "\n",
        "print('Cumulative variance explained by 50 principal components: {}'.format(np.sum(pca_50.explained_variance_ratio_)))\n",
        "# perform tsne on 50 components\n",
        "time_start = time.time()\n",
        "\n",
        "\n",
        "fashion_pca_tsne = TSNE(random_state=RS).fit_transform(pca_result_50)\n",
        "\n",
        "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2HvhLmEA-ah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fashion_scatter(fashion_pca_tsne, y_subset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqfk3460dGYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3T9eTgFHje3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}